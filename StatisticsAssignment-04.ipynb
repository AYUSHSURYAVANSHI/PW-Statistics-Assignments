{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (705129398.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with \n",
    "# an example.\n",
    "# Answer :-\n",
    "# The Probability Mass Function (PMF) and Probability Density Function (PDF) are mathematical concepts used to describe the probability distribution of a random variable in the context of probability and statistics. These functions provide a way to understand and quantify the likelihood of different outcomes for discrete and continuous random variables, respectively.\n",
    "\n",
    "# Probability Mass Function (PMF):\n",
    "\n",
    "# The PMF is used to describe the probability distribution of a discrete random variable. It assigns probabilities to each possible outcome or value of the random variable.\n",
    "# It is defined for each specific value of the random variable, and the sum of the probabilities for all possible values must equal 1.\n",
    "# The PMF is represented as a function, often denoted as P(X = x), where X is the random variable, and x represents a specific outcome.\n",
    "# Example:\n",
    "# Let's consider a simple example of rolling a fair six-sided die. The random variable X represents the outcome, which can be 1, 2, 3, 4, 5, or 6. The PMF for this case would look like:\n",
    "\n",
    "# P(X = 1) = 1/6\n",
    "# P(X = 2) = 1/6\n",
    "# P(X = 3) = 1/6\n",
    "# P(X = 4) = 1/6\n",
    "# P(X = 5) = 1/6\n",
    "# P(X = 6) = 1/6\n",
    "\n",
    "# Probability Density Function (PDF):\n",
    "\n",
    "# The PDF is used to describe the probability distribution of a continuous random variable. Instead of assigning probabilities to specific values, it assigns probabilities to ranges or intervals of values.\n",
    "# The PDF represents the likelihood of the random variable falling within a particular range of values.\n",
    "# Unlike the PMF, the PDF can have values greater than 1. To find the probability of an event occurring within a specific range, you need to integrate the PDF over that range.\n",
    "# Example:\n",
    "# Let's consider a continuous random variable, such as the height of people in a population. The PDF for this variable might be a normal distribution (bell-shaped curve). In this case, the PDF might be represented by a function like:\n",
    "\n",
    "# f(x) = (1 / (σ * √(2π))) * e^(-(x - μ)^2 / (2σ^2))\n",
    "\n",
    "# In this equation, μ represents the mean of the height distribution, σ represents the standard deviation, and \"x\" is the height value. The PDF provides the probability density for any specific height value \"x\" and can be used to calculate the probability of a person's height falling within a specific range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?\n",
    "# # Answer :-\n",
    "# The Cumulative Density Function (CDF), often denoted as F(x), is a fundamental concept in probability and statistics. It provides a way to describe the cumulative probability of a random variable being less than or equal to a specific value. The CDF is used to understand and analyze the distribution of both discrete and continuous random variables.\n",
    "\n",
    "# Here's an explanation and example of a CDF:\n",
    "\n",
    "# Definition:\n",
    "\n",
    "# The CDF of a random variable X is defined as F(x) = P(X ≤ x), where \"X\" is the random variable, and \"x\" is a specific value.\n",
    "# In simpler terms, the CDF tells us the probability that the random variable is less than or equal to a given value \"x.\"\n",
    "# Example:\n",
    "# Let's consider a simple example using a fair six-sided die. The random variable X represents the outcome of rolling the die, which can be 1, 2, 3, 4, 5, or 6. The CDF for this case would look like:\n",
    "\n",
    "# F(x) = P(X ≤ x)\n",
    "\n",
    "# F(1) = P(X ≤ 1) = 1/6\n",
    "# F(2) = P(X ≤ 2) = 2/6\n",
    "# F(3) = P(X ≤ 3) = 3/6\n",
    "# F(4) = P(X ≤ 4) = 4/6\n",
    "# F(5) = P(X ≤ 5) = 5/6\n",
    "# F(6) = P(X ≤ 6) = 6/6 = 1\n",
    "\n",
    "# The CDF shows the cumulative probability of rolling the die and getting a value less than or equal to \"x.\" For example, F(3) represents the probability that the outcome is 3 or less, which is 3/6 or 0.5.\n",
    "\n",
    "# Why CDF is used:\n",
    "\n",
    "# Visualization: CDFs provide a visual representation of the entire distribution of a random variable. They can be useful for understanding the spread, central tendency, and overall shape of the distribution.\n",
    "\n",
    "# Probability Calculation: CDFs allow you to calculate the probability that a random variable falls within a particular range. For instance, to find P(a ≤ X ≤ b), you can subtract F(a) from F(b).\n",
    "\n",
    "# Comparison: CDFs allow for easy comparison of different probability distributions. You can compare CDFs to assess which distribution has a higher probability of specific outcomes.\n",
    "\n",
    "# Quantiles: CDFs are useful for finding quantiles, such as the median, quartiles, and percentiles, which help summarize the distribution and identify key values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Q3: What are some examples of situations where the normal distribution might be used as a model? \n",
    "# # Explain how the parameters of the normal distribution relate to the shape of the distribution.\n",
    "# Answer :-\n",
    "# The normal distribution, also known as the Gaussian distribution, is a commonly used probability distribution in statistics. It is characterized by its bell-shaped curve and is widely used to model various phenomena in the natural and social sciences. The parameters of the normal distribution are the mean (μ) and the standard deviation (σ), and they play a crucial role in shaping the distribution. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "# Height of Individuals: The heights of a population often follow a normal distribution. In this case, the mean (μ) represents the average height of the population, and the standard deviation (σ) measures the spread or variation in heights. Most people will be around the mean height, and fewer will be much taller or shorter.\n",
    "\n",
    "# IQ Scores: IQ scores are often assumed to follow a normal distribution. The mean IQ is set to 100, and the standard deviation is set to 15. This means that most people have IQ scores close to 100, and fewer have scores that deviate significantly from the mean.\n",
    "\n",
    "# Measurement Errors: When making precise measurements in experimental sciences, the errors involved are often assumed to follow a normal distribution. The mean represents the true value, and the standard deviation quantifies the precision or accuracy of the measurements.\n",
    "\n",
    "# Stock Market Returns: Daily or monthly returns of stocks or financial instruments are often assumed to be normally distributed. The mean return represents the average gain or loss, and the standard deviation indicates the volatility of the investment.\n",
    "\n",
    "# Test Scores: In educational testing, the scores on standardized tests like the SAT or GRE often approximate a normal distribution. The mean score represents the average performance, and the standard deviation characterizes the variation in scores.\n",
    "\n",
    "# Natural Phenomena: Many natural phenomena, such as the distribution of particle velocities in a gas, can be approximated by the normal distribution. In these cases, the mean and standard deviation relate to the average velocity and the spread of velocities, respectively.\n",
    "\n",
    "# The shape of the normal distribution is determined by its parameters as follows:\n",
    "\n",
    "# Mean (μ): The mean is the center of the distribution, and it determines where the peak of the bell-shaped curve is located. Shifting the mean to the left or right will change the position of the peak without affecting the symmetry of the curve.\n",
    "\n",
    "# Standard Deviation (σ): The standard deviation controls the spread or dispersion of the distribution. A smaller σ results in a narrower and taller curve, while a larger σ leads to a broader and shorter curve. It quantifies how much data points tend to deviate from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal \n",
    "# # Distribution. \n",
    "# # Answer :-\n",
    "# The normal distribution, also known as the Gaussian distribution, is of significant importance in statistics and data analysis due to its numerous applications in real-life scenarios. Here are some key reasons for its importance and a few real-life examples where it is commonly observed:\n",
    "\n",
    "# Importance of Normal Distribution:\n",
    "\n",
    "# Commonality: The normal distribution is one of the most common probability distributions found in nature and social sciences. Many real-world phenomena tend to approximate a normal distribution, making it a useful model for a wide range of situations.\n",
    "\n",
    "# Mathematical Simplicity: The normal distribution is mathematically well-defined and easily described by its parameters, the mean (μ) and standard deviation (σ). This simplicity makes it a preferred choice for statistical analysis.\n",
    "\n",
    "# Central Limit Theorem: The Central Limit Theorem states that the sampling distribution of the sample means, from a population with any underlying distribution, approaches a normal distribution as the sample size increases. This theorem is essential in inferential statistics.\n",
    "\n",
    "# Statistical Inference: The normal distribution underpins many statistical tests and methods, such as hypothesis testing, confidence intervals, and regression analysis. It simplifies the calculations and provides theoretical foundations for these procedures.\n",
    "\n",
    "# Real-Life Examples of Normal Distribution:\n",
    "\n",
    "# Height of Individuals: The heights of individuals in a population often follow a normal distribution. The mean height represents the average height of the population, and the standard deviation characterizes the variation in heights.\n",
    "\n",
    "# IQ Scores: IQ scores are designed to follow a normal distribution, with a mean of 100 and a standard deviation of 15. This allows for easy comparison and interpretation of intelligence scores.\n",
    "\n",
    "# Exam Scores: In educational settings, the scores on standardized tests like the SAT or GRE tend to approximate a normal distribution. The mean score represents the average performance, and the standard deviation measures the spread of scores.\n",
    "\n",
    "# Errors in Measurements: In experimental sciences and engineering, measurement errors often follow a normal distribution. The mean represents the true value, and the standard deviation quantifies the precision or accuracy of measurements.\n",
    "\n",
    "# Stock Market Returns: Daily or monthly returns of stocks or financial instruments are often assumed to be normally distributed. The mean return represents the average gain or loss, and the standard deviation indicates the volatility of the investment.\n",
    "\n",
    "# Residuals in Regression Analysis: In regression analysis, the distribution of residuals (the differences between observed and predicted values) is often assumed to be normal. This assumption is crucial for valid statistical inference.\n",
    "\n",
    "# Natural Phenomena: Various natural phenomena, such as the distribution of particle velocities in a gas, the distribution of birth weights, and the distribution of human body temperatures, can be approximated by a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli \n",
    "# # Distribution and Binomial Distribution?\n",
    "# # Answer :-\n",
    "# Bernoulli Distribution:\n",
    "# The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes: success and failure. It is named after Swiss mathematician Jacob Bernoulli. In the Bernoulli distribution, a random variable X takes on the value of 1 for success with probability p and the value of 0 for failure with probability (1 - p), where 0 <= p <= 1. The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "# P(X = 1) = p\n",
    "# P(X = 0) = 1 - p\n",
    "\n",
    "# Example of Bernoulli Distribution:\n",
    "# A classic example of the Bernoulli distribution is modeling the outcome of a single flip of a fair coin. Let X be the random variable representing the outcome of the coin flip. If we define \"Heads\" as success (X = 1) and \"Tails\" as failure (X = 0), the probability of getting Heads (success) is p = 0.5 (for a fair coin), and the probability of getting Tails (failure) is 1 - p = 0.5.\n",
    "\n",
    "# Difference between Bernoulli Distribution and Binomial Distribution:\n",
    "\n",
    "# Number of Trials:\n",
    "\n",
    "# Bernoulli Distribution: It models a single trial or experiment with two possible outcomes.\n",
    "# Binomial Distribution: It models the number of successes in a fixed number of independent and identical Bernoulli trials (experiments).\n",
    "# Random Variable:\n",
    "\n",
    "# Bernoulli Distribution: It has a single random variable, which represents the outcome of a single trial (X = 1 for success, X = 0 for failure).\n",
    "# Binomial Distribution: It has a random variable \"X\" representing the number of successes in a fixed number of trials.\n",
    "# Parameters:\n",
    "\n",
    "# Bernoulli Distribution: It has one parameter, \"p,\" which represents the probability of success in a single trial.\n",
    "# Binomial Distribution: It has two parameters, \"n\" (the number of trials) and \"p\" (the probability of success in each trial).\n",
    "# Probability Function:\n",
    "\n",
    "# Bernoulli Distribution: It has a simple PMF with two possible outcomes: P(X = 1) = p and P(X = 0) = 1 - p.\n",
    "# Binomial Distribution: It has a more complex PMF that describes the number of successes in \"n\" trials. The PMF is given by the binomial coefficient and involves multiple terms.\n",
    "# Use Cases:\n",
    "\n",
    "# Bernoulli Distribution: It is used for modeling single events with two possible outcomes, such as coin flips (Heads/Tails), success/failure in a single trial, or the presence/absence of a particular event.\n",
    "# Binomial Distribution: It is used for modeling the number of successes in a fixed number of repeated Bernoulli trials, such as the number of heads in multiple coin flips, the number of successful sales in a fixed number of attempts, or the number of defective items in a batch of products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset \n",
    "# is normally distributed, what is the probability that a randomly selected observation will be greater \n",
    "# than 60? Use the appropriate formula and show your calculations.\n",
    "# Answer :-\n",
    "# The Z-score for a value x in a normal distribution can be found using the following formula:\n",
    "# z = (x - μ) / σ\n",
    "# where μ represents the population mean, σ represents the population standard deviation, and x represents any given data point.\n",
    "# where μ is the population mean, σ is the standard deviation, and x is the given value.\n",
    "# To find the probability that a random observation from this dataset will be greater than 60, we need to calculate the cumulative\n",
    "# To find the probability that a random observation from this dataset will be greater than 60, we need to calculate the area under the curve between\n",
    "# To find the probability that a random observation from this dataset will be greater than 60, we need to calculate the cumulative\n",
    "# To find the probability that a random observation from this dataset will be greater than 60, we need to use the cumulative\n",
    "# To find the probability P(X > 60), we need to calculate the area under the curve from negative infinity to 60 on\n",
    "# To find the probability P(X > 60), we need to use the cumulative distribution function (CDF) of the normal distribution\n",
    "# To find the probability that a random sample from this distribution will have a score higher than 60,\n",
    "# we use the cumulative distribution function (CDF), denoted as F(x). This gives us the proportion\n",
    "# of values less than or equal to x. We are interested in finding P(X > 60), so we subtract the CDF at\n",
    "# 60 from 1.\n",
    "# The CDF for a normal distribution is given by:\n",
    "# F(x) = Φ((x - μ)/σ)\n",
    "# where Φ is the cumulative distribution function of the standard normal distribution.\n",
    "# Since our data set has a mean of 50 and a standard deviation of 10, we substitute these into the formulas above.\n",
    "# Since our data set has a mean of 50 and a standard deviation of 10, we substitute these into the equation:\n",
    "# F(x) = Φ((60 - 50)/10)\n",
    "# After calculating the cumulative distribution function for 60, we subtract it from one to get the required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Q7: Explain uniform Distribution with an example.\n",
    "# Answer :-\n",
    "\n",
    "# The uniform distribution is a probability distribution in which all outcomes within a given range are equally likely. It is characterized by a constant probability density function (PDF) within the range and zero probability outside that range. In simple terms, every possible value in the specified interval has the same likelihood of occurring. This distribution is commonly used in situations where each outcome has an equal chance of happening.\n",
    "\n",
    "# Mathematical Representation:\n",
    "# A continuous uniform distribution over the interval [a, b] is typically denoted as U(a, b). The PDF of the uniform distribution is defined as:\n",
    "\n",
    "# f(x) = 1 / (b - a) for a <= x <= b\n",
    "# f(x) = 0 elsewhere\n",
    "\n",
    "# Where:\n",
    "\n",
    "# \"a\" is the lower bound of the range.\n",
    "# \"b\" is the upper bound of the range.\n",
    "# Example of Uniform Distribution:\n",
    "\n",
    "# Let's consider an example to illustrate the uniform distribution:\n",
    "\n",
    "# Suppose you have a random number generator that produces values between 0 and 1 with equal probability. This situation can be modeled by a continuous uniform distribution.\n",
    "\n",
    "# The range of possible outcomes is [0, 1].\n",
    "# The probability of generating any specific value between 0 and 1 is constant and equal to 1 / (1 - 0) = 1.\n",
    "# In this case, the probability density function of the uniform distribution would look like this:\n",
    "\n",
    "# f(x) = 1 for 0 <= x <= 1\n",
    "# f(x) = 0 elsewhere\n",
    "\n",
    "# This means that any value between 0 and 1 is equally likely to be generated by the random number generator.\n",
    "\n",
    "# Uniform distributions are also used in various applications, such as modeling the uniform distribution of birth dates over a year when studying the birthday paradox, or modeling the distribution of data values when there is no specific reason to favor one value over another within a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Q8: What is the z score? State the importance of the z score.\n",
    "# Answer :-\n",
    "# The z-score, also known as the standard score, is a measure of how many standard deviations a data point is away from the mean (average) of a dataset. It is a standardized score that allows for the comparison of data points from different distributions. The z-score is an important concept in statistics and data analysis for several reasons:\n",
    "\n",
    "# 1. Standardization: Z-scores standardize data, making it easier to compare values from different datasets or populations. By converting data into z-scores, you can assess how unusual or common a particular observation is relative to the rest of the data.\n",
    "\n",
    "# 2. Interpretation: Z-scores provide a common scale for interpretation. A z-score of 0 indicates that the data point is exactly at the mean, a positive z-score means the data point is above the mean, and a negative z-score means the data point is below the mean.\n",
    "\n",
    "# 3. Outlier Detection: Z-scores help in identifying outliers in a dataset. Data points with z-scores that are significantly higher or lower than 0 (typically beyond ±2 or ±3 standard deviations) may be considered outliers.\n",
    "\n",
    "# 4. Hypothesis Testing: Z-scores are used in hypothesis testing. When comparing sample means or proportions to population parameters, z-scores are used to calculate test statistics and assess the statistical significance of results.\n",
    "\n",
    "# 5. Probability Calculations: Z-scores are used to find probabilities in a standard normal distribution (with a mean of 0 and a standard deviation of 1). Given a z-score, you can use a z-table or statistical software to find the probability that a data point falls below or above that z-score.\n",
    "\n",
    "# 6. Confidence Intervals: Z-scores play a role in constructing confidence intervals for population parameters. Confidence intervals provide a range within which a population parameter is likely to fall.\n",
    "\n",
    "# 7. Quality Control: In manufacturing and quality control, z-scores are used to assess product quality and identify defects. Data points with z-scores outside certain tolerance limits may be considered unacceptable.\n",
    "\n",
    "# 8. Grading and Evaluation: Z-scores are commonly used in educational assessments and standardized tests. They allow for the comparison of individual performance against a standardized distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.\n",
    "# Answer :-\n",
    "# The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the behavior of the sampling distribution of the sample mean (or other sample statistics) as the sample size increases, regardless of the shape of the population distribution. The CLT has significant importance in statistical inference and data analysis. Here are the key aspects and significance of the Central Limit Theorem:\n",
    "\n",
    "# Definition of the Central Limit Theorem:\n",
    "\n",
    "# The Central Limit Theorem can be stated as follows:\n",
    "\n",
    "# Given a random sample of n observations drawn from a population with a finite mean (μ) and a finite standard deviation (σ), as n becomes sufficiently large, the sampling distribution of the sample mean (X̄) will be approximately normally distributed, regardless of the shape of the population distribution. This normal distribution will have a mean equal to the population mean (μ) and a standard deviation equal to the population standard deviation divided by the square root of the sample size (σ/√n).\n",
    "\n",
    "# Significance of the Central Limit Theorem:\n",
    "\n",
    "# Approximation of Normal Distribution: The Central Limit Theorem is significant because it allows us to make inferences about population parameters using the normal distribution, even when the population distribution is not normally distributed. This approximation is crucial for hypothesis testing and constructing confidence intervals.\n",
    "\n",
    "# Applicability to Real-World Data: Many real-world datasets are not normally distributed. However, the CLT implies that as long as the sample size is sufficiently large, the sampling distribution of the sample mean will be close to normal. This is a practical tool for data analysis.\n",
    "\n",
    "# Large Sample Sizes: The CLT provides a basis for justifying the use of parametric statistical methods (e.g., t-tests, ANOVA, regression) even when dealing with non-normally distributed data, as long as the sample size is large.\n",
    "\n",
    "# Quality Control and Process Improvement: In quality control and process improvement, the CLT allows for the use of control charts and other statistical tools to monitor and improve processes, assuming large sample sizes.\n",
    "\n",
    "# Reliability of Estimation: The CLT ensures that the sample mean is a reliable estimator of the population mean, especially with larger sample sizes. This is essential for making accurate inferences and predictions.\n",
    "\n",
    "# Statistical Inference: The CLT is a cornerstone of statistical inference. It forms the basis for techniques like hypothesis testing, confidence intervals, and margin of error calculations.\n",
    "\n",
    "# Teaching and Understanding Statistics: The CLT is a fundamental concept in statistics education. It helps students understand the properties of sample means and provides a bridge between sample statistics and population parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q10: State the assumptions of the Central Limit Theorem.\n",
    "# Answer :-\n",
    "# The Central Limit Theorem (CLT) is a fundamental concept in statistics, but it relies on specific assumptions to hold true. These assumptions are essential for the CLT to be applicable. Here are the main assumptions of the Central Limit Theorem:\n",
    "\n",
    "# Random Sampling: The samples used in the application of the CLT must be obtained through random sampling. This means that each data point has an equal and independent chance of being included in the sample. Non-random sampling can lead to biased results.\n",
    "\n",
    "# Independence: The observations within the sample must be independent of each other. This means that the value of one observation should not depend on or be influenced by the values of other observations. In the context of time series data, observations taken at different time points should be far enough apart to be considered independent.\n",
    "\n",
    "# Sample Size: The sample size (n) must be sufficiently large. While there is no strict rule for what constitutes \"sufficiently large,\" a commonly used guideline is that n should be greater than or equal to 30. However, the larger the sample size, the better the CLT approximation.\n",
    "\n",
    "# Population Distribution: The population from which the samples are drawn should have a finite mean (μ) and a finite standard deviation (σ). If the population is highly skewed or has heavy tails, larger sample sizes may be necessary for the CLT to hold.\n",
    "\n",
    "# Random Variable's Distribution: The distribution of the random variable under study does not need to be normal. The CLT applies to a wide range of population distributions, including non-normally distributed populations. However, the distribution should not be extremely non-normal or heavily skewed.\n",
    "\n",
    "# Finite Variances: The population must have finite variances (σ^2) for the CLT to apply. Infinite variances can lead to unstable or undefined sampling distributions.\n",
    "\n",
    "# Replacement (for Sampling with Replacement): If sampling is done with replacement, the CLT still holds, but it's more stringent. Sampling without replacement is more common in practice and is often assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
